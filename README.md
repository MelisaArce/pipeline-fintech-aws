<div style="display: flex; justify-content: space-between; align-items: center; width: 100%;">
  
  <div style="flex: 1;">
    <h1>üöÄ Proyecto ETL - An√°lisis Bancario Berka</h1>
    <p>
      Este repositorio contiene el <em>pipeline</em> de Extracci√≥n, Transformaci√≥n y Carga (ETL)
      desarrollado en <strong>PySpark</strong> para procesar los datos hist√≥ricos del dataset bancario Berka,
      transform√°ndolos de una capa <strong>Raw</strong> a una capa <strong>Processed</strong> y, finalmente, a una capa
      <strong>Curated</strong> optimizada para el an√°lisis.
      <br><br>
      El proyecto est√° dise√±ado para ser ejecutado de forma local usando <strong>LocalStack</strong> (simulando servicios de AWS)
      y en producci√≥n usando <strong>AWS Glue</strong>.
    </p>
  </div>

  <div style="flex-shrink: 0;">
    <img src="./img/logo-berka.png" alt="Logo Berka" width="150">
  </div>

</div>

## üìò Documentaci√≥n Completa

- üèóÔ∏è [Arquitectura del Pipeline](docs/arquitectura.md)
- üîç [EDA Completo](docs/eda.md)
- üìä [An√°lisis de Negocio](docs/analisis.md)
- üé® [Metodolog√≠a del Dashboard](docs/metodologia_dashboard.md)
- üè¶ [Dashboard Conclusiones](docs/dashboard_conclusiones.md)
- üí∞ [Optimizacion de Costos](docs/optimizacion_costos.md)

## üèóÔ∏è Estructura del Proyecto

| Elemento                                    | Tipo          | Descripci√≥n                                                                                        |
| ------------------------------------------- | ------------- | -------------------------------------------------------------------------------------------------- |
| **`/glue-jobs`**                            | Directorio    | Scripts PySpark listos para ejecutarse en AWS Glue.                                                |
| ‚îú‚îÄ‚îÄ `raw_to_processed.py`                   | Script        | Limpieza, estandarizaci√≥n y conversi√≥n RAW ‚Üí PROCESSED.                                            |
| ‚îú‚îÄ‚îÄ `processed_to_curated.py`               | Script        | Feature Engineering, m√©tricas financieras y creaci√≥n del modelo dimensional (PROCESSED ‚Üí CURATED). |
| ‚îî‚îÄ‚îÄ `berka_curated_to_rds.py`               | Script        | Carga final de la capa Curated hacia MySQL RDS.                                                    |
| **`/local`**                                | Directorio    | Configuraci√≥n del entorno local (LocalStack + Docker) y datos de ingesta.                          |
| ‚îú‚îÄ‚îÄ `/data_original`                        | Directorio    | Archivos CSV originales utilizados en el EDA y la ingesta RAW.                                     |
| ‚îú‚îÄ‚îÄ `EDA.ipynb`                             | Notebook      | Notebook del an√°lisis exploratorio inicial (EDA).                                                  |
| ‚îú‚îÄ‚îÄ `run_job_local.sh`                      | Script        | Orquestador local completo del pipeline en Docker.                                                 |
| ‚îú‚îÄ‚îÄ `docker-compose.yml`                    | Configuraci√≥n | Ambiente local que emula AWS (LocalStack, MySQL, Spark).                                           |
| **`/docs`**                                 | Directorio    | Documentaci√≥n del proyecto.                                                                        |
| ‚îú‚îÄ‚îÄ `arquitectura.md`                       | Documento     | Explicaci√≥n de la arquitectura y dise√±o del Data Lake House.                                       |
| ‚îú‚îÄ‚îÄ `metodologia_dashboard.md`              | Documento     | L√≥gica anal√≠tica, definiciones de KPIs y gobernanza de QuickSight.                                 |
| ‚îú‚îÄ‚îÄ `eda.md`                                | Documento     | EDA documentado que justifica decisiones del ETL y del dashboard.                                  |
| ‚îî‚îÄ‚îÄ *otros documentos a√±adidos al proyecto* | ‚Äî             | (mockups, decisiones t√©cnicas, etc.)                                                               |
| **`/athena-queries`**                       | Directorio    | Consultas SQL utilizadas durante an√°lisis intermedio o auditor√≠as.                                 |
| ‚îî‚îÄ‚îÄ `analisis_v4.sql`                          | Script SQL    | Ejecuciones de prueba/validaci√≥n sobre capas procesadas.                                           |
| **`/MySql`**                                | Directorio    | Scripts SQL para creaci√≥n de tablas destino en MySQL RDS.                                          |
| **`/quicksight`**                           | Directorio    | Mockups, PDFs y artefactos de dise√±o de tableros.                                                  |
| **`/img`**                                  | Directorio    | Logos, diagramas e im√°genes usadas en documentaci√≥n.                                               |
| **`README.md`**                             | Documentaci√≥n | Visi√≥n general del proyecto, explicaci√≥n funcional y t√©cnica del pipeline.                         |
| **`deploy_to_aws_v2.sh`**                   | Script        | Despliegue automatizado de infraestructura y c√≥digo con CloudFormation.                            |
| **`cloudformation_template_v6.yaml`**       | IaC           | Plantilla que define S3, RDS, IAM Roles, VPC y servicios AWS asociados.                            |


## 1\. Configuraci√≥n de Entorno Local (Docker Compose)

El archivo `docker-compose.yml` define los servicios necesarios para simular el entorno de nube (**LocalStack**) y la base de datos de destino (**MySql**), permitiendo el desarrollo y prueba de los Jobs Spark de forma aislada.

### `docker-compose.yml`

```yaml
version: '3.8'

services:
  # 1. Base de Datos de Destino (Data Warehouse)
  mysql:
    image: mysql:8.0
    container_name: berka_mysql_db 
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: berka_warehouse
    ports:
      - "3306:3306" 
    volumes:
      - mysql_data:/var/lib/mysql
    networks:
      - berka_network
    healthcheck:
        test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p$$MYSQL_ROOT_PASSWORD"]
        interval: 5s
        timeout: 5s
        retries: 5
        start_period: 20s

  # 2. LocalStack (Simulador de AWS S3)
  localstack:
    container_name: localstack_berka
    image: localstack/localstack:latest
    ports:
      - "4566:4566"        # Puerto API de LocalStack
      - "4571:4571"
    environment:
      # Activar solo los servicios que se necesitan para ahorrar recursos
      SERVICES: s3,rds,iam
      # Configurar la regi√≥n y usar el hostname del contenedor
      AWS_DEFAULT_REGION: us-east-1
      DOCKER_HOST: unix:///var/run/docker.sock
    volumes:
      - ./data:/tmp/data  # Montar un volumen para archivos de inicializaci√≥n (opcional)
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks:
      - berka_network
    
  # 3. Contenedor del Cliente Spark (Para ejecutar spark-submit)
  spark-client:
    container_name: berka-spark-client
    # Imagen oficial de Spark para PySpark
    image: bitnami/spark:3.5.0
    command: ["tail", "-f", "/dev/null"] # Mantener el contenedor vivo
    environment:
      # Configuraci√≥n de Spark para conectar a LocalStack
      SPARK_LOCAL_IP: "spark-client"
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
    volumes:
      # Montar los scripts ETL y los datos RAW
      - ./jobs:/opt/bitnami/spark/jobs
      - ./data/raw:/data/raw
    networks:
      - berka_network
    depends_on:
      db:
        condition: service_healthy
      localstack:
        condition: service_started

networks:
  berka_network:
    driver: bridge
```

## 2\. Scripts ETL de PySpark (Jobs)

Ambos scripts usan el patr√≥n de **Configuraci√≥n Adaptativa** que detecta la variable de entorno `EXECUTION_MODE`.

  * **Modo `GLUE` (Default):** Usa las librer√≠as de `awsglue`, obtiene par√°metros de `getResolvedOptions`, y usa el protocolo `s3://`.
  * **Modo `LOCAL`:** Inicializa una `SparkSession` con la configuraci√≥n de `s3a://localhost:4566` y usa par√°metros *hardcodeados*.

-----

### 2.1. `raw_to_processed.py` (Raw ‚Üí Processed)

Este Job se encarga de la limpieza y estandarizaci√≥n inicial de los archivos CSV brutos.

#### üéØ Tareas Principales

  * **Carga Adaptativa:** Usa el protocolo `s3a://` en local y `s3://` en AWS.
  * **Limpieza de Nombres:** Normaliza todos los nombres de columnas a *snake\_case* y elimina caracteres especiales/comillas.
  * **Conversi√≥n de Fechas:** Convierte el formato hist√≥rico AAMMDD a `yyyy-MM-dd` (asumiendo 19XX).
  * **Imputaci√≥n Funcional:** Rellena valores nulos (`NULL`, `vac√≠o`, `-`) en columnas categ√≥ricas (`k_symbol`, `operation`) con etiquetas de negocio como `NO_SYMBOL` o `UNKNOWN`.
  * **Tipado:** Convierte columnas a los tipos de datos correctos (`IntegerType`, `DoubleType`, `DateType`).
  * **Salida:** Escribe los DataFrames limpios a la capa **Processed** en formato **Parquet**, particionando por columnas clave (`frequency`, `type`, `status`).

-----

### 2.2. `processed_to_curated.py` (Processed ‚Üí Curated)

Este es el Job de **Feature Engineering** y Agregaci√≥n, construyendo el modelo dimensional final.

#### üéØ Tareas Principales

  * **Enriquecimiento Dimensional:**
      * **CLIENT:** Calcula `gender`, `age_at_1998` y `age_segment` a partir del `birth_number`.
      * **LOAN:** Calcula `loan_end_date`, el *flag* binario `is_risky`, y ratios financieros.
      * **ACCOUNT:** Se enriquece con datos demogr√°ficos de `DISTRICT`, el `client_id` del propietario (`OWNER`) y la antig√ºedad de la cuenta.
  * **Feature Engineering Avanzado:**
      * **TRANSACTIONS:** Calcula el `initial_balance` usando funciones de ventana (`lag`) y agrega m√©tricas rodantes (ej., `avg_trans_amount_3m`).
      * Crea *flags* de *outliers* basados en el percentil 95.
  * **Agregaci√≥n de Hechos (Facts):**
      * Crea `fact_account_transactions` con m√©tricas globales y promedio mensual de transacciones.
      * Crea `fact_account_summary` (la tabla estrella principal) consolidando todas las dimensiones y hechos a nivel de cuenta, incluyendo la creaci√≥n del `customer_segment` (Premium, Loan, Card, Basic).
  * **Resoluci√≥n de Ambig√ºedad:** Se incluye la correcci√≥n para renombrar `df_card.type` a **`type_card`** para evitar el error `[AMBIGUOUS_REFERENCE]` al unir con `df_disp`.

-----
### 2.3. berka_curated_to_rds.py (Curated ‚Üí RDS/MySQL)

Este es el Job de Ingesta y Persistencia, la etapa final del pipeline que mueve los modelos de datos dimensionales y las tablas de hechos (Facts) desde el Data Lake (capa Curated en S3) hacia el Data Warehouse relacional (RDS/MySQL). Este destino es el punto de consumo para herramientas de Business Intelligence (BI) y aplicaciones.

#### üéØ Tareas Principales

* **Carga de Tablas Finales:** 
     * Carga todas las tablas dimensionales (dim_client, dim_account, dim_loan, etc.) y las tablas de hechos(fact_account_summary, fact_transactions, etc.) generadas por el job processed_to_curated.py desde la capa Curated de S3.
* **Limpieza y Adaptaci√≥n para SQL (clean_dataframe_for_mysql):**
      * Tipado: Convierte tipos de datos de Spark que pueden ser problem√°ticos en MySQL (ej., DecimalType a DoubleType).
      * Normalizaci√≥n de Nombres: Elimina caracteres especiales (guiones -, espacios, puntos .) de los nombres de columnas y asegura el snake_case para la base de datos relacional.
* **Conexi√≥n Adaptativa:**
      * Utiliza los par√°metros de conexi√≥n JDBC (JDBC_URL, JDBC_USER, JDBC_PASSWORD) obtenidos de manera segura: mediante getResolvedOptions en modo GLUE (producci√≥n) o a trav√©s de variables de entorno y valores hardcodeados en modo LOCAL (desarrollo).
* **Persistencia Optimizada (write_to_mysql):**

      * Escribe cada DataFrame a su tabla MySQL correspondiente en modo overwrite (reemplazo total de la tabla).
      * Aplica optimizaciones JDBC como batchsize, isolationLevel (READ_UNCOMMITTED) y rewriteBatchedStatements para una transferencia de datos eficiente y r√°pida.
      * Incluye un paso de Validaci√≥n Post-escritura para verificar que el conteo de filas en Spark (df.count()) coincide con el conteo de filas en la tabla MySQL despu√©s de la carga, asegurando la integridad de los datos.

-----

## 3\. Gu√≠a de Despliegue en AWS Glue (Producci√≥n)

Para ejecutar estos Jobs en AWS, solo necesitas subir el script y configurar el Job de Glue.

### üìù Requerimientos AWS

1.  **S3:** Los buckets deben estar configurados para albergar las capas **Raw**, **Processed**, y **Curated**.
2.  **IAM Role:** Necesitas un Rol de IAM para AWS Glue con las siguientes pol√≠ticas:
      * `AWSGlueServiceRole` (para Glue en general).
      * **S3 R/W** (Lectura/Escritura) para los prefijos `RAW_PREFIX`, `PROCESSED_PREFIX`, y `CURATED_PREFIX`.
      * **Secret Manager R/W** (si vas a usar credenciales seguras para RDS).
3.  **VPC/Subnet/Security Group (Solo para Job de RDS):** El Job de RDS necesitar√° ejecutarse dentro de la VPC que contenga la instancia de RDS.

### ‚öôÔ∏è Configuraci√≥n del Job de Glue

Al crear un nuevo Job de Glue, usar√°s los siguientes **Par√°metros de Job** (en la pesta√±a "Job details" o "Configuraci√≥n"):

| Clave (Key) | Descripci√≥n | Ejemplo |
| :--- | :--- | :--- |
| `--S3_BUCKET` | Nombre del bucket principal de S3. | `berka-data-lake-prod` |
| `--RAW_PREFIX` | Prefijo de la capa RAW. | `raw/berka/` |
| `--PROCESSED_PREFIX` | Prefijo de la capa PROCESSED. | `processed/berka/` |
| `--CURATED_PREFIX` | Prefijo de la capa CURATED. | `curated/berka/` |
| `--JOB_NAME` | Nombre para identificar el Job. | `berka-etl-raw-processed` |

El script autom√°ticamente usar√° el protocolo `s3://` y las credenciales del Rol de IAM, ya que la variable `EXECUTION_MODE` por defecto ser√° `GLUE`.

-----

## 4\. ‚öôÔ∏è Detalle de Orquestaci√≥n y Despliegue (Scripts Shell)

Esta secci√≥n explica la secuencia de comandos utilizada para ejecutar y validar los Jobs ETL tanto en el entorno de desarrollo local como en el entorno de producci√≥n de AWS Glue.

### 4.1. Gu√≠a de Ejecuci√≥n Local (`run_job_local.sh`)

Este *script* es el **orquestador local** que gestiona la configuraci√≥n de Docker, la carga de datos en el Data Lake simulado (LocalStack) y la ejecuci√≥n secuencial de los Jobs PySpark, culminando con la validaci√≥n de datos en MySQL.

| Fase | Tarea Principal | Comandos Clave | Prop√≥sito |
| :--- | :--- | :--- | :--- |
| **0 & 1. Preparaci√≥n** | Levanta los contenedores Docker y limpia directorios locales (`processed`, `curated`) que podr√≠an tener errores de permisos. | `docker compose up -d`, `sudo rm -rf...` | Asegura un estado limpio y reproducible para la ejecuci√≥n. |
| **2 & 3. Health Checks** | Espera a que los servicios cr√≠ticos (`LocalStack S3` y `MySQL`) est√©n operativos antes de continuar. | `curl... grep -q '"s3": "available"'`, `docker exec... mysqladmin ping` | Evita errores de conexi√≥n al intentar ejecutar Spark antes de que las dependencias est√©n listas (fundamental en Docker). |
| **4. Carga RAW** | Crea el *bucket* de S3 simulado y sincroniza los archivos CSV de la carpeta local (`./data/raw/berka/`) al *path* de S3 (`s3://berka-data-lake/raw/berka/`). | `aws s3 mb`, `aws s3 sync` | Simula la ingesta inicial de datos brutos en el Data Lake. |
| **5, 7 & 9. Ejecuci√≥n ETL** | Ejecuta secuencialmente los tres Jobs PySpark (Raw ‚Üí Processed ‚Üí Curated ‚Üí RDS) utilizando el comando `spark-submit` dentro del contenedor `spark-client`. | `docker exec... spark-submit --packages...` | Lanza el *pipeline* de transformaci√≥n, asegurando que se utilizan los paquetes JDBC y Hadoop AWS necesarios para la conexi√≥n. |
| **6 & 8. Verificaci√≥n** | Verifica que los Jobs intermedios crearon archivos en S3 y descarga una copia de las capas **Processed** y **Curated** al host local. | `aws s3 ls...`, `aws s3 sync...` | Permite al desarrollador inspeccionar la calidad y el formato Parquet de los datos generados. |
| **10. Validaci√≥n Final** | Ejecuta consultas SQL directamente en el contenedor de MySQL para validar el conteo de filas, la calidad de datos (g√©nero, riesgo) y el formato de las tablas cargadas. | `docker exec... mysql -e "SELECT..."` | Confirma que el *pipeline* ha cargado la base de datos de destino con la integridad esperada. |
| **Cleanup** (Trap) | Detiene y elimina los contenedores Docker en caso de √©xito o fallo del *script*. | `docker compose down` | Libera recursos del sistema despu√©s de la ejecuci√≥n. |

-----

### 4.2. Gu√≠a de Despliegue en AWS Glue (`deploy_aws.sh`)

Este *script* est√° dise√±ado para un despliegue de **Infraestructura como C√≥digo (IaC)**. Su objetivo es crear todos los recursos de AWS (S3, RDS, IAM Roles, Security Groups, etc.) a trav√©s de **CloudFormation** y luego cargar los artefactos necesarios para la ejecuci√≥n.

| Fase | Tarea Principal | Comando Clave | Prop√≥sito |
| :--- | :--- | :--- | :--- |
| **1 & 2. Validaciones** | Verifica la existencia de AWS CLI y credenciales, y obtiene informaci√≥n de la cuenta (ID, IP p√∫blica). | `aws sts get-caller-identity`, `curl https://api.ipify.org` | Previene fallos de despliegue por falta de configuraci√≥n y obtiene la IP para configurar el acceso a RDS. |
| **3 & 4. CloudFormation** | Crea el *Stack* de CloudFormation, provisionando la VPC, el RDS y los Roles de IAM (incluyendo el Rol para Glue). Luego espera a que la creaci√≥n se complete y obtiene los *outputs* clave. | `aws cloudformation create-stack...`, `aws cloudformation wait...` | Establece el ambiente de producci√≥n completo antes de cargar cualquier c√≥digo o dato. |
| **5. Subir Scripts** | Sube los tres Jobs PySpark (`raw_to_processed.py`, etc.) al *bucket* de S3 creado por CloudFormation (en el *path* `/scripts`). | `aws s3 cp "$SCRIPTS_DIR/$JOB"...` | Prepara los artefactos de c√≥digo para que los Jobs de AWS Glue puedan ser definidos y ejecutados. |
| **6. Subir Datos RAW** | Sincroniza el directorio local de datos originales (`./data_original`) al *path* de entrada de la capa RAW en S3 (`s3://$BUCKET_NAME/raw/berka/`). | `aws s3 sync "$RAW_DATA_DIR"...` | Asegura que el *pipeline* tenga los datos brutos disponibles para iniciar el procesamiento. |
| **7. Instrucciones** | Imprime los comandos finales de AWS CLI que el usuario debe ejecutar **manualmente** despu√©s de que el *script* haya finalizado. | `echo "aws glue start-job-run..."` | **El *script* despliega la infraestructura y el c√≥digo, pero el usuario debe iniciar el flujo de ejecuci√≥n y el Crawler de Glue.** |

#### üìù Secuencia de Ejecuci√≥n de Jobs en Producci√≥n (Manual)

Una vez que el *script* de despliegue finaliza, el flujo se controla mediante la consola de AWS Glue o la CLI:

1.  **Ejecutar Job 1 (Transformaci√≥n):** RAW ‚Üí PROCESSED.
    ```bash
    aws glue start-job-run --job-name <tu_nombre>-raw-to-processed
    ```
2.  **Ejecutar Job 2 (Feature Engineering):** PROCESSED ‚Üí CURATED.
    ```bash
    aws glue start-job-run --job-name <tu_nombre>-processed-to-curated
    ```
3.  **Ejecutar Crawler:** Catalogar los datos Parquet de la capa CURATED en el AWS Glue Data Catalog.
    ```bash
    aws glue start-crawler --name <tu_nombre>-curated-crawler
    ```
4.  **Ejecutar Job 3 (Carga a RDS):** CURATED ‚Üí RDS (MySQL).
    ```bash
    aws glue start-job-run --job-name <tu_nombre>-curated-to-rds
    ```