üöÄ Proyecto ETL - An√°lisis Bancario Berka

Este repositorio contiene el *pipeline* de Extracci√≥n, Transformaci√≥n y Carga (ETL) desarrollado en **PySpark** para procesar los datos hist√≥ricos del dataset bancario Berka, transform√°ndolos de una capa **Raw** a una capa **Processed** y, finalmente, a una capa **Curated** optimizada para el an√°lisis.

El proyecto est√° dise√±ado para ser ejecutado de forma local usando **LocalStack** (simulando AWS S3) y en producci√≥n usando **AWS Glue**.

## üèóÔ∏è Estructura del Proyecto

```
/berka-etl-pipeline
‚îú‚îÄ‚îÄ docker-compose.yml              # Configuraci√≥n de los contenedores (Spark, LocalStack, PostgreSQL)
‚îú‚îÄ‚îÄ spark-submit-local.sh           # Script para ejecutar los jobs en modo LocalStack
‚îú‚îÄ‚îÄ raw_csv_transform_w_local.py    # Job ETL: Raw (CSV) -> Processed (Parquet)
‚îú‚îÄ‚îÄ curated_job.py                  # Job ETL: Processed -> Curated (S3)
‚îî‚îÄ‚îÄ rds_load_job.py                 # Job ETL: Curated -> RDS (Pr√≥ximo paso)
```

## 1\. Configuraci√≥n de Entorno Local (Docker Compose)

El archivo `docker-compose.yml` define los servicios necesarios para simular el entorno de nube (**LocalStack**) y la base de datos de destino (**PostgreSQL**), permitiendo el desarrollo y prueba de los Jobs Spark de forma aislada.

### `docker-compose.yml`

```yaml
version: '3.8'

services:
  # 1. Base de Datos de Destino (Data Warehouse)
  db:
    image: postgres:15-alpine
    container_name: berka-postgres-db
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: berka_dw
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - berka_network

  # 2. LocalStack (Simulador de AWS S3)
  localstack:
    container_name: localstack_berka
    image: localstack/localstack:latest
    ports:
      - "4566:4566"        # Puerto API de LocalStack
      - "4571:4571"
    environment:
      # Activar solo los servicios que se necesitan para ahorrar recursos
      SERVICES: s3,rds,iam
      # Configurar la regi√≥n y usar el hostname del contenedor
      AWS_DEFAULT_REGION: us-east-1
      DOCKER_HOST: unix:///var/run/docker.sock
    volumes:
      - ./data:/tmp/data  # Montar un volumen para archivos de inicializaci√≥n (opcional)
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks:
      - berka_network
    
  # 3. Contenedor del Cliente Spark (Para ejecutar spark-submit)
  spark-client:
    container_name: berka-spark-client
    # Imagen oficial de Spark para PySpark
    image: bitnami/spark:3.5.0
    command: ["tail", "-f", "/dev/null"] # Mantener el contenedor vivo
    environment:
      # Configuraci√≥n de Spark para conectar a LocalStack
      SPARK_LOCAL_IP: "spark-client"
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
    volumes:
      # Montar los scripts ETL y los datos RAW
      - ./jobs:/opt/bitnami/spark/jobs
      - ./data/raw:/data/raw
    networks:
      - berka_network
    depends_on:
      db:
        condition: service_healthy
      localstack:
        condition: service_started

networks:
  berka_network:
    driver: bridge
```

### üìã Pasos de Ejecuci√≥n Local

1.  **Levantar Contenedores:**
    ```bash
    docker compose up -d
    ```
2.  **Copiar datos Raw a S3 simulado (LocalStack):**
    Una vez que los contenedores est√©n corriendo, debes copiar tus archivos CSV (`account.csv`, `client.csv`, etc.) del directorio local `./data/raw` al *bucket* `berka-data-lake` en LocalStack.
3.  **Ejecutar los Jobs ETL:** Usa el script auxiliar `spark-submit-local.sh` dentro del contenedor `spark-client`.

## 2\. Scripts ETL de PySpark (Jobs)

Ambos scripts usan el patr√≥n de **Configuraci√≥n Adaptativa** que detecta la variable de entorno `EXECUTION_MODE`.

  * **Modo `GLUE` (Default):** Usa las librer√≠as de `awsglue`, obtiene par√°metros de `getResolvedOptions`, y usa el protocolo `s3://`.
  * **Modo `LOCAL`:** Inicializa una `SparkSession` con la configuraci√≥n de `s3a://localhost:4566` y usa par√°metros *hardcodeados*.

-----

### 2.1. `raw_csv_transform_w_local.py` (Raw ‚Üí Processed)

Este Job se encarga de la limpieza y estandarizaci√≥n inicial de los archivos CSV brutos.

#### üéØ Tareas Principales

  * **Carga Adaptativa:** Usa el protocolo `s3a://` en local y `s3://` en AWS.
  * **Limpieza de Nombres:** Normaliza todos los nombres de columnas a *snake\_case* y elimina caracteres especiales/comillas.
  * **Conversi√≥n de Fechas:** Convierte el formato hist√≥rico AAMMDD a `yyyy-MM-dd` (asumiendo 19XX).
  * **Imputaci√≥n Funcional:** Rellena valores nulos (`NULL`, `vac√≠o`, `-`) en columnas categ√≥ricas (`k_symbol`, `operation`) con etiquetas de negocio como `NO_SYMBOL` o `UNKNOWN`.
  * **Tipado:** Convierte columnas a los tipos de datos correctos (`IntegerType`, `DoubleType`, `DateType`).
  * **Salida:** Escribe los DataFrames limpios a la capa **Processed** en formato **Parquet**, particionando por columnas clave (`frequency`, `type`, `status`).

-----

### 2.2. `curated_job.py` (Processed ‚Üí Curated)

Este es el Job de **Feature Engineering** y Agregaci√≥n, construyendo el modelo dimensional final.

#### üéØ Tareas Principales

  * **Enriquecimiento Dimensional:**
      * **CLIENT:** Calcula `gender`, `age_at_1998` y `age_segment` a partir del `birth_number`.
      * **LOAN:** Calcula `loan_end_date`, el *flag* binario `is_risky`, y ratios financieros.
      * **ACCOUNT:** Se enriquece con datos demogr√°ficos de `DISTRICT`, el `client_id` del propietario (`OWNER`) y la antig√ºedad de la cuenta.
  * **Feature Engineering Avanzado:**
      * **TRANSACTIONS:** Calcula el `initial_balance` usando funciones de ventana (`lag`) y agrega m√©tricas rodantes (ej., `avg_trans_amount_3m`).
      * Crea *flags* de *outliers* basados en el percentil 95.
  * **Agregaci√≥n de Hechos (Facts):**
      * Crea `fact_account_transactions` con m√©tricas globales y promedio mensual de transacciones.
      * Crea `fact_account_summary` (la tabla estrella principal) consolidando todas las dimensiones y hechos a nivel de cuenta, incluyendo la creaci√≥n del `customer_segment` (Premium, Loan, Card, Basic).
  * **Resoluci√≥n de Ambig√ºedad:** Se incluye la correcci√≥n para renombrar `df_card.type` a **`type_card`** para evitar el error `[AMBIGUOUS_REFERENCE]` al unir con `df_disp`.

-----

## 3\. Gu√≠a de Despliegue en AWS Glue (Producci√≥n)

Para ejecutar estos Jobs en AWS, solo necesitas subir el script y configurar el Job de Glue.

### üìù Requerimientos AWS

1.  **S3:** Los buckets deben estar configurados para albergar las capas **Raw**, **Processed**, y **Curated**.
2.  **IAM Role:** Necesitas un Rol de IAM para AWS Glue con las siguientes pol√≠ticas:
      * `AWSGlueServiceRole` (para Glue en general).
      * **S3 R/W** (Lectura/Escritura) para los prefijos `RAW_PREFIX`, `PROCESSED_PREFIX`, y `CURATED_PREFIX`.
      * **Secret Manager R/W** (si vas a usar credenciales seguras para RDS).
3.  **VPC/Subnet/Security Group (Solo para Job de RDS):** El Job de RDS necesitar√° ejecutarse dentro de la VPC que contenga la instancia de RDS.

### ‚öôÔ∏è Configuraci√≥n del Job de Glue

Al crear un nuevo Job de Glue, usar√°s los siguientes **Par√°metros de Job** (en la pesta√±a "Job details" o "Configuraci√≥n"):

| Clave (Key) | Descripci√≥n | Ejemplo |
| :--- | :--- | :--- |
| `--S3_BUCKET` | Nombre del bucket principal de S3. | `berka-data-lake-prod` |
| `--RAW_PREFIX` | Prefijo de la capa RAW. | `raw/berka/` |
| `--PROCESSED_PREFIX` | Prefijo de la capa PROCESSED. | `processed/berka/` |
| `--CURATED_PREFIX` | Prefijo de la capa CURATED. | `curated/berka/` |
| `--JOB_NAME` | Nombre para identificar el Job. | `berka-etl-raw-processed` |

El script autom√°ticamente usar√° el protocolo `s3://` y las credenciales del Rol de IAM, ya que la variable `EXECUTION_MODE` por defecto ser√° `GLUE`.