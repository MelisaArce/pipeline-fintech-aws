<!-- Banner: logo a la derecha, t√≠tulo a la izquierda -->
<div style="display: flex; justify-content: space-between; align-items: center; width: 100%;">
  <div>
    <h1>üèóÔ∏è Arquitectura del Proyecto ETL ‚Äì Berka Fintech</h1>
    <p>
    En este documento presento la arquitectura que dise√±√© y ejecut√© para el procesamiento anal√≠tico del hist√≥rico bancario Berka, siguiendo un enfoque Data Lakehouse sobre AWS. Toda la soluci√≥n est√° orientada a lograr un pipeline reproducible, escalable y seguro, que permita transformar datos crudos en informaci√≥n confiable para an√°lisis y visualizaci√≥n ejecutiva.

  Mi dise√±o integra almacenamiento en S3, procesamiento distribuido con AWS Glue, persistencia relacional en RDS MySQL y consumo anal√≠tico desde Athena y QuickSight. Este modelo fue la base de todas las decisiones posteriores en mi ETL, desde la estructura de carpetas hasta la l√≥gica del dashboard.
    </p>
  </div>
  <img src="../img/logo-berka.png" alt="logo berka" width="160" style="border-radius: 8px;">
</div>

## üìê Diagrama de Arquitectura

![arquitectura completa](../img/Arquitectura-berka.drawio.png)

La arquitectura se compone de cinco capas:

1. **Ingesta**: Archivos CSV hist√≥ricos como fuente principal.
2. **Data Lake (S3)**: Tres niveles de calidad ‚Üí *Raw*, *Processed*, *Curated*.
3. **Procesamiento (AWS Glue)**: Tres Jobs Spark que ejecutan limpieza, enriquecimiento y carga final.
4. **Data Warehouse (RDS MySQL)**: Base relacional para dashboards y exploraci√≥n r√°pida.
5. **Capa Anal√≠tica**: Athena + QuickSight.

Esta separaci√≥n por capas me permiti√≥ mantener trazabilidad y gobernanza durante todo el ciclo del dato.
---

# 1. üéØ Visi√≥n General de la Arquitectura

![arquitectura resumida](../img/berka_pipeline_fixed_v1.png)

Desde el inicio decid√≠ trabajar con un patr√≥n **Data Lakehouse**, porque me permit√≠a combinar:

* **Escalabilidad y bajo costo** de S3 como zona central de datos
* **Procesamiento distribuido** en AWS Glue (PySpark)
* **Consumo relacional** en RDS MySQL para dashboards de baja latencia
* **Consultas ad-hoc** desde Athena sobre archivos Parquet optimizados

Este enfoque me permiti√≥ tener un pipeline robusto, modular y f√°cil de desplegar con IaC.

---

# 3. üîÑ Flujo de Datos End-to-End (Pipeline ETL)

Para orquestar el flujo, implement√© **tres Jobs de AWS Glue** desarrollados en PySpark. Cada uno refleja una etapa clara del proceso de calidad del dato.

## 3.1. üóÉÔ∏è Origen de Datos

La fuente del proyecto es un conjunto de CSV bancarios hist√≥ricos (`account.csv`, `client.csv`, `trans.csv`, etc.).
Todos se almacenan inicialmente en la ruta:

```
s3://<bucket>/raw/berka/
```

---

## 3.2. üßπ Fase 1 ‚Äî Raw ‚Üí Processed (Estandarizaci√≥n)

**Job:** `raw_to_processed.py`
**Objetivo:** convertir los datos crudos en datos limpios y tipados.

**Transformaciones clave:**

* Normalizaci√≥n de nombres de columnas (snake_case)
* Eliminaci√≥n de caracteres err√≥neos
* Cast de tipos (fechas, enteros, decimales)
* Conversi√≥n a **Parquet** para mejorar performance

**Salida:**
`processed/berka/` ‚Üí mismos campos, pero con coherencia estructural.

---

## 3.3. üß† Fase 2 ‚Äî Processed ‚Üí Curated (Enriquecimiento)

**Job:** `processed_to_curated.py`
**Objetivo:** construir un modelo dimensional listo para an√°lisis.

**Transformaciones realizadas:**

* Joins entre transacciones, cuentas y clientes
* C√°lculo de edad ‚Üí bucketizaci√≥n de clientes
* Features financieros: ingresos, egresos, ratio de riesgo
* Tablas anal√≠ticas finales

  * `dim_customer`
  * `fact_transactions`
  * `dim_accounts`
  * entre otras

**Salida:**
`curated/berka/` ‚Üí Data Marts listos para consumo BI.

---

## 3.4. üè¶ Fase 3 ‚Äî Curated ‚Üí RDS (Persistencia Anal√≠tica)

**Job:** `curated_to_rds.py`

Decid√≠ cargar la capa Curated en MySQL RDS para brindar:

* Baja latencia en dashboards
* SQL transaccional optimizado
* Facilidad para QuickSight

Incluye creaci√≥n autom√°tica de tablas + carga incremental/batch.

---

# 4. üéõÔ∏è Infraestructura como C√≥digo (IaC) ‚Äî CloudFormation

Toda la arquitectura se despliega mediante una plantilla de **CloudFormation**, lo que asegura reproducibilidad. Esto fue clave para que el proyecto pueda levantarse en cualquier entorno sin configuraciones manuales.

### Recursos que provisiono con el template:

| Categor√≠a      | Recurso                                             | Uso y decisi√≥n t√©cnica                               |
| -------------- | --------------------------------------------------- | ---------------------------------------------------- |
| **S3**         | Bucket Data Lake (+ prefijos raw/processed/curated) | Base del Lakehouse, escalable y econ√≥mica            |
| **Glue**       | 3 Jobs Spark + IAM Role                             | Procesamiento distribuido sin servidores             |
| **RDS**        | MySQL 8.0 (db.t3.micro)                             | Data Warehouse para BI                               |
| **Crawler**    | Crawling de Curated                                 | Permite consultas desde Athena                       |
| **Networking** | Security Groups + Subnets + VPC Endpoint            | Comunicaci√≥n segura Glue ‚Üî RDS y acceso privado a S3 |

### Seguridad y red

Implement√©:

* **Security group self-referencing** para Glue
* **VPC Endpoint para S3** ‚Üí sin tr√°fico por internet
* **Acceso a RDS restringido a mi IP real** (extra√≠da autom√°ticamente por el deploy script)

Este dise√±o fortalece la seguridad sin agregar complejidad administrativa.

---

# 5. üìä Capa Anal√≠tica (Athena + QuickSight)

### **Athena**

La utilic√© para:

* Validar la calidad de los Parquet Curated
* Ejecutar el an√°lisis SQL de mi archivo `analisis.md`
* Explorar datos sin cargar RDS

Athena consume los mismos datos de Curated pero sin afectar ambientes productivos.

### **QuickSight**

Lo conect√© a:

* **RDS MySQL**, como fuente principal para dashboards
* Athena (para exploraci√≥n o m√©tricas ad-hoc)

Esto me permiti√≥ construir KPIs de riesgo, actividad de clientes y transacciones sospechosas.

---

# 6. üöÄ Despliegue Automatizado ‚Äî Script `deploy_to_aws_v2.sh`

Desarroll√© este script para automatizar el 80% del proceso de despliegue. Su rol es:

1. Validar el entorno (AWS CLI + credenciales)
2. Obtener la IP del equipo para configurar acceso a RDS
3. Crear el stack de CloudFormation
4. Esperar outputs del stack
5. Subir los scripts PySpark a S3
6. Subir los datos RAW
7. Mostrar los comandos manuales que completan la ejecuci√≥n

### Flujo manual final

Una vez desplegado:

```bash
aws glue start-job-run --job-name <name>-raw-to-processed
aws glue start-job-run --job-name <name>-processed-to-curated
aws glue start-crawler --name <name>-curated-crawler
aws glue start-job-run --job-name <name>-curated-to-rds
```

Esto garantiza que todos los pasos sean intencionales, controlados y reproducibles.

---

# 7. üß© Conclusi√≥n T√©cnica

El dise√±o de esta arquitectura me permiti√≥:

* Trabajar con buenas pr√°cticas reales de un Data Engineer
* Separar calidad de datos por capas
* Mantener un pipeline trazable, seguro y escalable
* Integrar ETL, Data Lake, DWH y BI en un √∫nico flujo
* Tener un deploy 100% automatizable

Esta soluci√≥n representa un Data Lakehouse moderno, accesible, completamente funcional y alineado con escenarios reales del mundo laboral.


