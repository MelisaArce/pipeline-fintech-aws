<!-- Banner: logo a la derecha, t√≠tulo a la izquierda -->
<div style="display: flex; justify-content: space-between; align-items: center; width: 100%;">
  <div>
    <h1>üèóÔ∏è Arquitectura del Proyecto ETL ‚Äì Berka Fintech</h1>
    <p>
    En este documento presento la arquitectura que dise√±√© y ejecut√© para el procesamiento anal√≠tico del hist√≥rico bancario Berka, siguiendo un enfoque Data Lakehouse sobre AWS. Toda la soluci√≥n est√° orientada a lograr un pipeline reproducible, escalable y seguro, que permita transformar datos crudos en informaci√≥n confiable para an√°lisis y visualizaci√≥n ejecutiva.

  Mi dise√±o integra almacenamiento en S3, procesamiento distribuido con AWS Glue, persistencia relacional en RDS MySQL y consumo anal√≠tico desde Athena y QuickSight. Este modelo fue la base de todas las decisiones posteriores en mi ETL, desde la estructura de carpetas hasta la l√≥gica del dashboard.
    </p>
  </div>
  <img src="../img/logo-berka.png" alt="logo berka" width="160" style="border-radius: 8px;">
</div>

## üìê Diagrama de Arquitectura

![arquitectura completa](../img/Arquitectura-berka.drawio.png)

La arquitectura se compone de cinco capas:

1. **Ingesta**: Archivos CSV hist√≥ricos como fuente principal.
2. **Data Lake (S3)**: Tres niveles de calidad ‚Üí *Raw*, *Processed*, *Curated*.
3. **Procesamiento (AWS Glue)**: Tres Jobs Spark que ejecutan limpieza, enriquecimiento y carga final.
4. **Data Warehouse (RDS MySQL)**: Base relacional para dashboards y exploraci√≥n r√°pida.
5. **Capa Anal√≠tica**: Athena + QuickSight.

Esta separaci√≥n por capas me permiti√≥ mantener trazabilidad y gobernanza durante todo el ciclo del dato.
---

# 1. üéØ Visi√≥n General de la Arquitectura

![arquitectura resumida](../img/berka_pipeline_fixed_v1.png)

Desde el inicio decid√≠ trabajar con un patr√≥n **Data Lakehouse**, porque me permit√≠a combinar:

* **Escalabilidad y bajo costo** de S3 como zona central de datos
* **Procesamiento distribuido** en AWS Glue (PySpark)
* **Consumo relacional** en RDS MySQL para dashboards de baja latencia
* **Consultas ad-hoc** desde Athena sobre archivos Parquet optimizados

Este enfoque me permiti√≥ tener un pipeline robusto, modular y f√°cil de desplegar con IaC.

---

# 3. üîÑ Flujo de Datos End-to-End (Pipeline ETL)

Para orquestar el flujo, implement√© **tres Jobs de AWS Glue** desarrollados en PySpark. Cada uno refleja una etapa clara del proceso de calidad del dato.

## 3.1. üóÉÔ∏è Origen de Datos

La fuente del proyecto es un conjunto de CSV bancarios hist√≥ricos (`account.csv`, `client.csv`, `trans.csv`, etc.).
Todos se almacenan inicialmente en la ruta:

```
s3://<bucket>/raw/berka/
```

---

## 3.2. üßπ Fase 1 ‚Äî Raw ‚Üí Processed (Estandarizaci√≥n)

**Job:** `raw_to_processed.py`
**Objetivo:** convertir los datos crudos en datos limpios y tipados.

**Transformaciones clave:**

* Normalizaci√≥n de nombres de columnas (snake_case)
* Eliminaci√≥n de caracteres err√≥neos
* Cast de tipos (fechas, enteros, decimales)
* Conversi√≥n a **Parquet** para mejorar performance

**Salida:**
`processed/berka/` ‚Üí mismos campos, pero con coherencia estructural.

---

## 3.3. üß† Fase 2 ‚Äî Processed ‚Üí Curated (Enriquecimiento)

**Job:** `processed_to_curated.py`
**Objetivo:** construir un modelo dimensional listo para an√°lisis.

**Transformaciones realizadas:**

* Joins entre transacciones, cuentas y clientes
* C√°lculo de edad ‚Üí bucketizaci√≥n de clientes
* Features financieros: ingresos, egresos, ratio de riesgo
* Tablas anal√≠ticas finales

  * `dim_customer`
  * `fact_transactions`
  * `dim_accounts`
  * entre otras

**Salida:**
`curated/berka/` ‚Üí Data Marts listos para consumo BI.

---

## 3.4. üè¶ Fase 3 ‚Äî Curated ‚Üí RDS (Persistencia Anal√≠tica)

**Job:** `curated_to_rds.py`

Decid√≠ cargar la capa Curated en MySQL RDS para brindar:

* Baja latencia en dashboards
* SQL transaccional optimizado
* Facilidad para QuickSight

Incluye creaci√≥n autom√°tica de tablas + carga incremental/batch.

---
Aqu√≠ ten√©s la versi√≥n **en primera persona**, con un tono **profesional y acad√©mico**, manteniendo toda la estructura y el contenido t√©cnico pero narrado desde tu experiencia directa:

---

# Esquema Estrella (*Star Schema*)

![arquitectura resumida](../img/Diagrama-er-estrella.drawio.png)

## 1. ‚≠êÔ∏è Modelo de Warehouse: Star Schema (Esquema Estrella)

En el *Data Warehouse* que dise√±√© sobre MySQL RDS, opt√© por implementar un modelo de **Esquema Estrella**. Eleg√≠ este enfoque porque ofrece una combinaci√≥n ideal de **simplicidad estructural y alto rendimiento**, especialmente √∫til para entornos de Inteligencia de Negocio (BI). Esto me permite asegurar que las consultas consumidas desde QuickSight sean consistentes, r√°pidas y eficientes.

| Caracter√≠stica                | Beneficio Clave                                                                                                                                                   |
| :---------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Optimizaci√≥n de Consultas** | Simplifico el SQL reduciendo la cantidad de *joins* necesarios, lo que mejora significativamente la latencia del *dashboard*.                                     |
| **Separaci√≥n de L√≥gica**      | Mantengo una clara separaci√≥n entre las **medidas** (hechos) y las **descripciones** (dimensiones), lo que facilita el mantenimiento y la comprensi√≥n del modelo. |
| **Escalabilidad**             | Puedo a√±adir nuevas dimensiones o m√©tricas sin requerir modificaciones estructurales profundas.                                                                   |

---

## 2. Tablas de Hechos (*Fact Tables*)

Las Tablas de Hechos constituyen el n√∫cleo del an√°lisis que constru√≠, ya que contienen los eventos medibles (montos, conteos y m√©tricas derivadas).

| Tabla de Hechos (3)             | Prop√≥sito de Negocio                                                                                                                                            | Granularidad                         |
| :------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------- |
| **`fact_transactions`**         | Registro cada movimiento financiero (ingresos, retiros y gastos). Esta tabla es clave para identificar anomal√≠as o comportamientos at√≠picos.                    | Transacci√≥n individual               |
| **`fact_account_transactions`** | Agrupo las m√©tricas por cuenta (suma de ingresos, egresos, ratio entre ambos, etc.). Esta tabla me permite medir la salud financiera desde un enfoque agregado. | Cuenta (vista agregada en el tiempo) |
| **`fact_loan_dispositions`**    | Capturo el estado de cada pr√©stamo, lo cual es fundamental para medir el cumplimiento, el *status* y calcular el **capital en incumplimiento**.                 | Pr√©stamo (estado final)              |

---

## 3. Tablas de Dimensi√≥n 

Las Tablas de Dimensi√≥n son las que me permiten contextualizar los hechos: responden a las preguntas *qui√©n*, *d√≥nde*, *cu√°ndo* y *c√≥mo*.

| Tabla de Dimensi√≥n (7)     | Contenido Clave                                                                                        | Relaci√≥n con Hechos                                              |
| :------------------------- | :----------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------- |
| **`dim_client`**           | Incluyo informaci√≥n demogr√°fica del cliente (g√©nero, segmentaci√≥n por edad).                           | Se relaciona con `fact_transactions` y `fact_loan_dispositions`. |
| **`dim_account`**          | Contiene los datos maestros de cada cuenta (antig√ºedad, frecuencia).                                   | Se relaciona con todas las tablas de hechos.                     |
| **`dim_district`**         | Agrego informaci√≥n geogr√°fica y socioecon√≥mica (regi√≥n, salario promedio, desempleo).                  | Se vincula con `dim_account`.                                    |
| **`dim_loan`**             | Registro las caracter√≠sticas del pr√©stamo (monto, duraci√≥n, fecha de inicio).                          | Relacionado con `fact_loan_dispositions`.                        |
| **`dim_card`**             | Describo el tipo de tarjeta asignada al cliente.                                                       | Apoya an√°lisis sobre transacciones espec√≠ficas.                  |
| **`dim_date`**             | Modelo la dimensi√≥n temporal (d√≠a, semana, mes, a√±o) para permitir an√°lisis evolutivos y estacionales. | Relacionada con todas las tablas de hechos.                      |
| **`dim_transaction_type`** | Incluye el contexto de la transacci√≥n (tipo de operaci√≥n).                                             | Relacionada con `fact_transactions`.                             |

---

### **Resumen**

El *Star Schema* que implement√© me permite que todo el trabajo de *Feature Engineering* realizado en las tablas de hechos pueda analizarse r√°pida y eficientemente bajo cualquier dimensi√≥n relevante: cliente, cuenta, geograf√≠a, tiempo o tipo de operaci√≥n. Este dise√±o asegura flexibilidad para nuevos an√°lisis y un rendimiento √≥ptimo para las herramientas de BI.

---

# 4. üéõÔ∏è Infraestructura como C√≥digo (IaC) ‚Äî CloudFormation

Toda la arquitectura se despliega mediante una plantilla de **CloudFormation**, lo que asegura reproducibilidad. Esto fue clave para que el proyecto pueda levantarse en cualquier entorno sin configuraciones manuales.

### Recursos que provisiono con el template:

| Categor√≠a      | Recurso                                             | Uso y decisi√≥n t√©cnica                               |
| -------------- | --------------------------------------------------- | ---------------------------------------------------- |
| **S3**         | Bucket Data Lake (+ prefijos raw/processed/curated) | Base del Lakehouse, escalable y econ√≥mica            |
| **Glue**       | 3 Jobs Spark + IAM Role                             | Procesamiento distribuido sin servidores             |
| **RDS**        | MySQL 8.0 (db.t3.micro)                             | Data Warehouse para BI                               |
| **Crawler**    | Crawling de Curated                                 | Permite consultas desde Athena                       |
| **Networking** | Security Groups + Subnets + VPC Endpoint            | Comunicaci√≥n segura Glue ‚Üî RDS y acceso privado a S3 |

### Seguridad y red

Implement√©:

* **Security group self-referencing** para Glue
* **VPC Endpoint para S3** ‚Üí sin tr√°fico por internet
* **Acceso a RDS restringido a mi IP real** (extra√≠da autom√°ticamente por el deploy script)

Este dise√±o fortalece la seguridad sin agregar complejidad administrativa.

---

# 5. üìä Capa Anal√≠tica (Athena + QuickSight)

### **Athena**

La utilic√© para:

* Validar la calidad de los Parquet Curated
* Ejecutar el an√°lisis SQL de mi archivo `analisis.md`
* Explorar datos sin cargar RDS

Athena consume los mismos datos de Curated pero sin afectar ambientes productivos.

### **QuickSight**

Lo conect√© a:

* **RDS MySQL**, como fuente principal para dashboards
* Athena (para exploraci√≥n o m√©tricas ad-hoc)

Esto me permiti√≥ construir KPIs de riesgo, actividad de clientes y transacciones sospechosas.

---

# 6. üöÄ Despliegue Automatizado ‚Äî Script `deploy_to_aws_v2.sh`

Desarroll√© este script para automatizar el 80% del proceso de despliegue. Su rol es:

1. Validar el entorno (AWS CLI + credenciales)
2. Obtener la IP del equipo para configurar acceso a RDS
3. Crear el stack de CloudFormation
4. Esperar outputs del stack
5. Subir los scripts PySpark a S3
6. Subir los datos RAW
7. Mostrar los comandos manuales que completan la ejecuci√≥n

### Flujo manual final

Una vez desplegado:

```bash
aws glue start-job-run --job-name <name>-raw-to-processed
aws glue start-job-run --job-name <name>-processed-to-curated
aws glue start-crawler --name <name>-curated-crawler
aws glue start-job-run --job-name <name>-curated-to-rds
```

Esto garantiza que todos los pasos sean intencionales, controlados y reproducibles.

---

# 7. üß© Conclusi√≥n T√©cnica

El dise√±o de esta arquitectura me permiti√≥:

* Trabajar con buenas pr√°cticas reales de un Data Engineer
* Separar calidad de datos por capas
* Mantener un pipeline trazable, seguro y escalable
* Integrar ETL, Data Lake, DWH y BI en un √∫nico flujo
* Tener un deploy 100% automatizable

Esta soluci√≥n representa un Data Lakehouse moderno, accesible, completamente funcional y alineado con escenarios reales del mundo laboral.


