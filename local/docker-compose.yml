services:
  localstack:
    image: localstack/localstack:latest
    container_name: localstack_berka
    user: "${USER_ID:-1000}:${GROUP_ID:-1000}"
    ports:
      - "4566:4566"
    environment:
      - SERVICES=s3
      - DEFAULT_REGION=us-east-1
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - localstack-data:/var/lib/localstack
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks:
      - berka_network 
  mysql:
    image: mysql:8.0
    container_name: berka_mysql_db 
    environment:
      MYSQL_ROOT_PASSWORD: password
      MYSQL_DATABASE: berka_warehouse
    ports:
      - "3306:3306" 
    volumes:
      - mysql_data:/var/lib/mysql
    networks:
      - berka_network
    healthcheck:
        test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p$$MYSQL_ROOT_PASSWORD"]
        interval: 5s
        timeout: 5s
        retries: 5
        start_period: 20s
  spark-submit-client:
    image: bitnami/spark:3.5.0
    container_name: spark_client_berka
    #user: "${USER_ID:-1000}:${GROUP_ID:-1000}" 
    environment:
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - AWS_DEFAULT_REGION=us-east-1
      - SPARK_OPTS=--conf spark.eventLog.enabled=false
      - EXECUTION_MODE=LOCAL
      - MYSQL_HOST=mysql 
      - MYSQL_PORT=3306
      - MYSQL_DATABASE=berka_warehouse
      - MYSQL_USER=root
      - MYSQL_PASSWORD=password
    volumes:
      - ../glue-jobs/raw_csv_transform_w_local.py:/opt/bitnami/spark/jobs/berka_etl_job.py
      - ../glue-jobs/curated_job.py:/opt/bitnami/spark/jobs/berka_curated_job.py 
      - ../glue-jobs/curated_to_rds.py:/opt/bitnami/spark/jobs/berka_rds_job.py 
      - ./data/raw/berka:/data/raw/berka
      - ./data/processed/berka:/data/processed/berka
      - ./data/curated/berka:/data/curated/berka
    depends_on:
      - localstack
      - mysql 
    networks:
      - berka_network
    command: tail -f /dev/null

volumes:
  localstack-data:
  mysql_data:

networks:
  berka_network:
    driver: bridge